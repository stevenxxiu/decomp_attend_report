\PassOptionsToPackage{pdfpagelabels=false}{hyperref}
\documentclass{article}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{hhline}
\usepackage[titletoc,title]{appendix}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}

\makeatletter
\newcommand*{\Appendixautorefname}{Appendix}
\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\newsavebox\CBox\def\textBF#1{\sbox\CBox{#1}\resizebox{\wd\CBox}{\ht\CBox}{\textbf{#1}}}\parindent=0pt
\sisetup{exponent-product = \cdot}
\setlength{\parskip}{0.8em}
\renewcommand\hyper@natlinkbreak[2]{#1}
\makeatother

\begin{document}

\title{Reproduction of A Decomposable Attention Model for Natural Language Inference}

\author{
  \textbf{Xingyi Xu} \\ University of Melbourne \\ \texttt{stevenxxiu@gmail.com} \and
  \textbf{Jey Han Lau} \\ IBM Research \\ \texttt{depthchargex@gmail.com} \and
  \textbf{Timothy Baldwin} \\ University of Melbourne \\ \texttt{tb@ldwin.net}
}
\maketitle

\begin{abstract}
Neural network models can be difficult to reproduce due to its many hyperparameters, initialization and training details. We attempted to reproduce the results of a paper ``A Decomposable Attention Model for Natural Language Inference'' for the stanford natural language inference dataset, using \texttt{tensorflow}. For the vanilla approach, we obtained an accuracy of 85.9\%, comparable to the paper's 86.3\%. However for the intra-sentence approach, we did not find an improvement, and instead found significant overfitting and obtained a lower accuracy of 85.7\%, in comparison to the paper's 86.8\%.
\end{abstract}

\section{Introduction}
The recent resurgence in AI is in large part due to the many successful neural network models. These models often have many hyperparameters, and can be initialized and trained in multiple ways. However, the focus on many papers has been just the model itself, with a minimal or neglected description of the details, possibly due to the fact that different initilization and training schemes can still achieve similar results. Hence it is useful to investigate whether we can reproduce neural network papers using its description alone, and if not, what can be done to improve reproducibility.

We investigate a paper in the space of natural language inference. This is the problem of given two sentences, determining whether sentence 2 entails (E), contradicts (C), or is neutral (N) to sentence 1. Examples are given in \autoref{table:examples}.

\begin{table}[htbp]\centering
\setlength\tabcolsep{2pt}
\begin{tabular}{|p{7.5cm}|p{7.5cm}|c|}
    \hline
    Sentence 1 & Sentence 2 & Label \\ \hhline{|===|}
    A man inspects the uniform of a figure in some East Asian country. & The man is sleeping & C \\ \hline
    An older and younger man smiling. & Two men are smiling and laughing at the cats playing on the floor. & N \\ \hline
    A black race car starts up in front of a crowd of people. & A man is driving down a lonely road. & C \\ \hline
    A soccer game with multiple males playing. & Some men are playing a sport. & E \\ \hline
    A smiling costumed woman is holding an umbrella. & A happy woman in a fairy costume holds an umbrella. & E \\ \hline
\end{tabular}
\caption{Examples from the Stanford Natural Langauge Inference (SNLI) dataset.}
\label{table:examples}
\end{table}

The paper ``A Decomposable Attention Model for Natural Language Inference'' \citep{parikh_decomposable_2016} is interesting in that despite being a model with a relatively low 580k parameters, it achieves a good claimed accuracy of 86.8\% on the SNLI dataset \citep{snli:emnlp2015}, which is not too far behind the current best model of 88.8\% accuracy, with 6.4m parameters. Another aspect is that the vanilla part of the model does not take any word ordering into account, however still manages to achieve a claimed accuracy of 86.3\%. For these reasons we decided to investigate this particular paper.

In the following sections, we give a description the vanilla and intra-sentence models, any ambiguities in the paper which slowed down our implementation, and a report of how we managed to implement the model.

\section{Models}
We found the model to be very well described, and the only part that may be unclear is the training objective's dropout being stated in the experimental section. We now describe the model for the purpose of being self-contained, and use the paper's notation.

Let the two input sentences be $\textbf{a}=(a_1,...,a_{l_a})$ and $\textbf{b}=(b_1,...,b_{l_b})$ with lengths $l_a$ and $l_b$, respectively. $a_i, b_j\in\mathbb{R}^d$ are word embeddings of dimension $d$ for word $i$ in sentence 1 and word $j$ in sentence 2, and we explicitly define them in the next section. Now let $\bar{a}_i = a_i, \bar{b}_i = b_i$. The reason we use $\bar{a}_i, \bar{b}_i$ instead is for convenience in describing the intra-sentence approach, where they are different to $a_i, b_i$.

For the first step, we soft-align each word with the other sentence. Let $F$ be a feed-forward network with ReLU activations, and define the unnormalized attention weights $e_{i,j} := F(\bar{a}_i)^T F(\bar{b}_j)$. We normalize and perform a weighted sum:

\begin{align*}
    \beta_i := \sum_{j=1}^{l_b} \frac{\exp(e_{i,j})}{\sum_{k=1}^{l_b}\exp(e_{i,k})} \bar{b}_j &&
    \alpha_j := \sum_{i=1}^{l_a} \frac{\exp(e_{i,j})}{\sum_{k=1}^{l_a}\exp(e_{k,j})} \bar{a}_i
\end{align*}

For the second step, we compare soft alignment with their corresponding sentence. Let $G$ be a feed-forward network with ReLU activations, and define $\mathbf{v}_{1,i} := G([\bar{a}_i, \beta_i]), \mathbf{v}_{2,j} := G([\bar{b}_j, \beta_j])$ for $i=1,...,l_a, j=1,...,l_b$, where $[\cdot, \cdot]$ denotes concatenation.

For the third and last step, we aggregate the comparison vectors by summation $\mathbf{v}_1 = \sum_{i=1}^{l_a}\mathbf{v}_{1,i}, \mathbf{v}_2 = \sum_{j=1}^{l_b}\mathbf{v}_{2,i}$. Let $H$ be a feed-forward network with ReLU activations with a final softmax classification layer, we define the prediction $\hat{y} := H([\mathbf{v}_1, \mathbf{v}_2])$. Here $\hat{y}\in\mathbb{R}^C$ for $C$ classes.

Now given training instance $n$, for the prediction $\hat{y}_c^{(n)}$ and true binary labels $y^{(n)}\in\{0, 1\}^C$, the training objective is the categorical cross-entropy loss $L = \frac{1}{N} \sum_{n=1}^N \sum_{c=1}^C y_c^{(n)} \log \hat{y}_c^{(n)}$.

\subsection{Intra-sentence attention}
In the intra-sentence attention modification to the model, $\bar{a}_i, \bar{b}_i$ are defined differently. The modification aligns a sentence to itself. Let $F_\text{intra}$ be a feed-forward network with ReLU activations, and define the unnormalized alignment weights $f_{i,j} := F_\text{intra}(\bar{a}_i)^T F_\text{intra}(\bar{a}_j)$. Now for words at positions $i, j$, a distance sensitive bias $d_{i-j}\in\mathbb{R}$ is added to the weights to provide the model with sequence information. The bias is defined s.t. $d_{i-j} = d_\text{far}$ for $|i-j| > 10$, where $d_\text{far}$ is a trained parameter. The self-aligned phrases are:

\[a'_i := \sum_{j=1}^{l_a} \frac{\exp(f_{i,j} + d_{i-j})}{\sum_{k=1}^{l_a} \exp(f_{i,k} + d_{i-k})} a_j\]

And in place of the word embeddings, we use: $\bar{a}_i := [a_i, a_i'], \bar{b}_i := [b_i, b_i']$.

\subsection{Ambiguities common to most neural network papers}
The total loss is described, but not the batch loss. We assume that the batch loss is the loss restricted to instances within the batch.

\section{Experimental results}
In our reproduction of the vanilla approach, we obtained an accuracy of 85.9\%, comparable to the paper's 86.3\%. However for the intra-sentence approach, we did not find an improvement, and instead found significant overfitting and obtained a lower accuracy of 85.7\%, in comparison to the paper's 86.8\%.

\subsection{Ambiguities in the paper}
What $F, F_\text{intra}, G, H$ are exactly is not specified in the paper. The paper mentioned $F$ to be a ``feed-forward network with ReLU activations'', and that the network size was ``$2$-layers, each with $200$ neurons''. It can be guessed that each of $F, F_\text{intra}, G, H$ are $2$-layer neural networks, each with $200$ neurons and ReLU activations, excluding $H$'s final linear layer. The total number of parameters then approximately matches that given in the paper. For the vanilla approach, this is $300 \cdot 200 + (201 \cdot 200 + 201 \cdot 200) + (401 \cdot 200 + 201 \cdot 200) + (401 \cdot 200 + 201 \cdot 200 + 201 \cdot 3) = 381803 \approx 382\text{K}$, and for the intra-sentence approach, this is $300 \cdot 200 + 21 + (201 \cdot 200 + 201 \cdot 200) + (401 \cdot 200 + 201 \cdot 200) + (801 \cdot 200 + 201 \cdot 200) + (401 \cdot 200 + 201 \cdot 200 + 201 \cdot 3) = 582224 \approx 582\text{K}$.

\subsection{Implementation details}
Due to our use of GPUs, it was easier for us to use one large batch size instead of the paper's ``10 asynchronous gradient-update threads''. We tried a similar setting with a batch size of $40$ with Adagrad, but we found that used Adam \citep{kingma_adam:_2014}, with a learning rate of $0.01$ with a batch size of $256$, converged and ran faster (\autoref{table:results}).

Usually when embedding vectors are trained, we only need to include words in the training set, but since embeddings are pretrained static GloVe vectors \citep{pennington2014glove}, we should include all words in the GloVe dataset. For efficiency this is the intersection between the set of GloVe words and all words in the SNLI dataset (this includes the training, validation and test sets). This detail can be missed in the implementation.

It may also be worth mentioning the way we implemented masking. We used a binary masking array for each batch. The masking array was used in the attend step (both intra and inter) to set the unnormalized weights being masked out to $-\infty$, so that these values become $0$ after the softmax function is run. The masking array was used again in the aggregate step, in which comparison vectors being masked out were set to $0$ before being summed over.

\subsection{Results}

\begin{table}[htbp]\centering
\begin{tabular}{|l|r|}
    \hline
    Method     & Accuracy \\ \hline
    \hline
\end{tabular}
\caption{Classification results for the SNLI dataset. XXX.}
\label{table:results}
\end{table}

\section{Conclusion}
We have described the process in which we attempted to reproduce the paper ``Reproduction of A Decomposable Attention Model for Natural Language Inference''. XXX. We hope that our findings can guide future paper authors in the area of deep learning to describe their models in a clearer fashion so that they are more easily reproducible.

\bibliography{main}{}
\bibliographystyle{apalike}

\end{document}
